---
title: "notes"
output: pdf_document
---


# On Modeling

A theory is an explanation, a model is an instantiation of that theory.

* Do you use models in explanations? Yes and No.

  + No in the sense that theory is the explanation. 
  + Yes in the sense that theories do not explain everything, they are "instantiations of the real world."
  
* Do you use models in description? Yes and No.

  + No when I just describe what I see in the data.
  + Yes when I evoke a statistical, computational, or any other type of model.




# Process or not


Weight goes up over time. 

1)

Not a process

2)

A description of the emergent properties of a process.
There is a process at play that determines weight going up over time, and this is a description of the emergent stuff.

stocks and flows, trend, growth, granger causality

Are these:

1) descriptions of the emergent properties of a process

or

2) descriptions, but not process. We can only use the term process when we provide a mechanistic explanation. If this is the case, then what do we call the first set of principles (e.g., growth, periodicity, magnitude)...

or

3) it depends. Growth/trend/gc can be used to explain or describe the observed data



3) what are process descriptions then? Or, can we only use the term process when we provide a mechanistic explanation? 

    + So we therefore have no ""

* Not a process

  + This phrase has no meaning
  + Your representation





# intro



We -- organizational psychologists -- are increasingly interested in process and dynamic phenomena. Longitudinal studies are becoming more prevalent in our literature and the number of time points they employ appears to be growing [@xu2018]. The empirical literature uses the term "dynamics" at expoentially larger rates in recent years [@deshon2012]. A majority of published methods literature now focuses on longitudinal data analysis [@aguinis2009], and there are now many great reviews on the conceptual and methodological issues related to process and dynamics [@wang2016; @beal2015; @shipp2015; @mitchell2001]. Moreover, this interest covers many content areas, including emotional labor [@bono2007; @beal2015; @gabriel2015], workplace stress and well-being [@ritter2016; @wang2014], organizatiional performance [@short2006], self-regulation [@hardy2018], newcomer adjustment [@kammeyer-meuller2013], justice and trust [@kaltianinen2017], leadership [@farh2018; @schaubroeck2016; @kalish2016], decision-making [@huang2012], team performance [@humphrey2017], counterproductive work behaviors [@meier2013], work-family conflict [@matthews2014], job satisfaction [@zablah2016], and team emergent states [@kozlowski2012]. In summary, explaining how a process functions appears to be of great interest to current organizational science. 

There are many ways to do so -- alternative representations that we might use when we want to describe sequences of events and their relationships. Just as different statistical models can be used to draw the same inference given the appropriate assumptions about the data generating process, we can use different forms of explanation to describe process. For example, Bandura [-@bandura] and Kuwabara et al. [-@kuwabara2018], respectively, explain self regulation and lay beliefs about networking with verbal theories, @denrell2009 presents a mathematical explanation of social impressions, and @vancouver2014 employ both mathematical and computational approaches to explain self-regulation. All of these authors use different techniques and forms of representation, but they are all trying to convey how the processes they study behave. 

In this paper, we present some of the fundamental principles researchers use to convey process. The principles come from a number of areas, including mathematics, systems theory, dynamics, and computational modeling, and they are all ways to represent and describe relationships over time at different levels of abstraction. For example, we could use a difference equation to explain the trajctory of one variable, or we could use terms like trend or cycles that describe the emergent behavior of the variable but through a different lens. What is important is that there are fundamental principles/concepts that go into to describing a process over time. Don't necessarily need to be causal; we can explain something and it can be causal or non causal. 

We provide several contributions in doing so. First, we believe our discussion will help researchers augment their current approach to explaining process. It can be helpful to be exposed to different approaches, provide ideas etc., and we hope our paper provides new ideas to seasoned researchers in this area. Second, although there is a substantial amount of literature on mathematical process principles and dynamics, it is sophisticated and technical. Much of this work is not easily accessible to researchers with the usual methodological and statistical background obtained from doctoral level training in OP/OB; we want to help distill it. Finally, we discuss ways to study process for researchers who may not want to want to develop sophisticated math or computational models. Some people claim that math is the only option. For example, Pearl (2009) states that any explanation "worthy of the title theory must be able to represent causal questions in some mathematical language" (p. 102). There is also some pressure to produce computational theories. For example, VANCOUVER COMP MODELS ARE BETTER; AND KOZLOWSKI COMP FRAMEWORKS ARE BETTER. But there are not many comp modelers in organizational psychology (vancouver orm); and the social sciences do not emphasize mathematics as much as some of the more physical sciences (vancouver orm). Moreoever, Renee Thom points out that sometimes qualitative representations produce more error than their quantitative counterparts but nonetheless are better clues to the underlying process. We do not claim that one approach is better or worse than another; we simply want to describe process principles from different domains to give researchers alternative ways of talking about, specifying, and representing process behavior. 

Below, we do these things. There are other excellent papers on aspects that we will not cover. Ployhart and Vandenberg discuss how to design and analyze a longitudinal study, Pitariu and Ployhart how to propose dynamic hypotheses, and Wang provides an overview of dynamic statistical models. In this paper, conversely, we focus solely on principles researchers use when they explain process.



after meeting

Here are all of the ways process is discussed. But they are talking about different things. Hollenbeck consensus building.

At what point in the reduction are you doing process? At what level of abstraction/reduction amd I explaining the process versus describing the emergent properties of the process?

Intro

There is increasing interest in process and dynamic phenomena in our science. For example

Some examples include. example 1. example 2. example 3 from different conetent areas doing different things.

All of the examples are interested in process, but notice that they are talking about different things. explain the differences. Which is an appropriate representation of process behavior?

The examples above were different forms of explanation from different content areas, now consider the same dilemma at different levels of abstraction. Weight gain. At what level of abstraction/reduction are we explaining the process versus describing the emergent properties of the process?

Our paper
Distinguish between explaining vs. describing process. 
Fundamentals for explaining process and fundamentals for describing process.
The many representations of process. Consensus build




# stocks and flows

Many organizational phenomena can be viewed as combinations of stocks and flows. Stocks: Affect [@sonnentag], helping behaviors, depletion, number of customers, justice perceptions, work-family conflict. Flows: turnover, stressful events, goal assignments. Sometimes the same thing can be expressed as both a stock and a flow, depending on how the researcher abstracts the situation. For example, the number of work tasks could be a stock, where it increases when we are given more assignments and decreases when we finish them. Or it could be a flow that leads into something like stress.

# stationarity

Modeling techniques make assumptions about the data generating process they attempt to capture, and a key assumption in dynamic models is stationarity, or the stability of properties in a time series. Stationarity subsumes two requirements: mean stationarity, which refers to a series with a constant mean, and variance stationary, which refers to a series with a constant variance. Almost all dynamic models used in the organizational literature are stationary models and assume the data they model are realizations of a stationary process. In simple terms, this means that we expect the properties (mean and variance) of a time series at time $t$ to be the same at time $t+1$. 

In general, we are interested in the relationships between one or more time series or panel trajectories. Acknowleding stationarity is critical for analyzing these relationships because two series that are non-stationary will be related regardless of the data generating process [@kuljanin2010; @braun2010]. That is, two (or more) independent variables that share no causal relationship will appear related in the observed data when they have trend. The first step in a dynamic analysis, then, is to check whether there is evidence of trend (i.e., non-stationarity).

The most common method for checking stationarity is the Dickey-Fuller (ADF) test [@dickey1979] in which the null hypothesis is that the time series contains a time-*dependent* error term. If the series is stationary, it will contain a time-*invariant* error term and thus the ADF significance test will be rejected. This test was designed in the econometrics literature where single time-series are more prevalent than panel data (repeated measures designs with many *N*), but equivalent tests are now available for data structures more consistent with those found in the organizational literature. 

Complete explanations of stationarity tests for both time-series and panel data are available in @jebb2017 and @braun2010. Because our interest here, consistent with the organizational literature, is on panel data and not single time-series relationships we use a panel version of the Dickey-Fuller test. 


# cointegration

In the current paper we assessed stationarity before moving to formal data modeling because Granger and Newbold [-@granger1974] demonstrated that regressions between independent non-stationary series will likely produce significant coefficient estimates and large $R^2$ values. Analyzing series with independent (i.e., unrelated/not causal) trends, random walks, or uneven variance across time, therefore, will likely result in spurious inference. It is also possible, however, that two non-stationary processes are related. We then need a way to assess that relationship without biasing our results in the manner presented by Granger and Newbold (1974). In a later paper, Granger [-@granger1986] showed that, under very specific circumstances, we can garner evidence for a relationship between two non-stationary series. Formally, two series, $x$ and $y$ are said to be cointegrated if 1) both are integrated of the same order, $I(d)$, and 2) they share a linear combination that is $I(0)$. In other words, there is a linear combination of $x$ and $y$ that is stationary (requirement #2) and they both require the same number of steps to make them stationary (requirement #1). 

The basis for cointegration is simple: do we have evidence for a relationship between series in a situation where we know spurious relationships are likely (i.e., non-stationarity)? Implementing cointegration techniques, however, is much more difficult. The analyst needs to be sensitive to the type of non-stationarity present in the series: trends or random walks. A random walk is defined as 

\begin{equation}
y_{t} = 1*y_{(t-1)} + e_{t}
\end{equation}

\noindent where the variable is exactly what it was at the prior time point but also accumulates error across time. These series may increase, decrease, or change directions at any instant. A series with trend is simply one that increases or decreases over time. Again, the steps required to demonstrate cointegration change if the analyst is dealing with trends or random walks. The analysis also changes if the variables are single-units or panels. Cointegration was developed for time-series data but panel techniques are also available now [@kao1999]. At this point we recommend ensuring data are stationary. Cointegration merits another paper entirely. 



## Granger Causality

X granger causes Y if Y can be better predicted by the histories of both X and Y than the history of Y alone. If lagged values of X help predict current values of Y in a forecast formed from lagged values of both X and Y, then X is said to Granger cause Y. We implement this notion by regressing eggs on lagged eggs and lagged chickens; if the coefficients on lagged chickens are significant as a group, then chickens cause eggs. A symetric regression tests the reverse causality. We perform the Granger causality tests using one to four lags. The number of lags in each equation is the same for eggs and chickens. To conclude that one of the two "came first," we must find unidirectional causality from one to the other. In other words, we must reject the noncausality of the one to the other and at the same time fail to reject the noncausality of the other to the one. If either both cause each other or neigher causes the other, the question will remain unanswered. Results reject that eggs do not Granger cause chickens. They provide no such reject of the hypothesis that chickens do not Granger cause eggs. Therefore, we conclude that eggs cause chickens. A better phrasing might be "temporally related" (Granger & Newbold, p. 225) -- Thurman and Fisher 1988 call it temporally ordered. 


# Simon example

Key
* Dynamics of the environment
  + position of goal-relevant objects in the environments changes and new goal objects appear as the agent moves
* Internal dynamics
  + internal states change and decay over time
* To understand behavior you need to know internal agent dynamics and the distribution of goal-relevant objects in the environment

# Process definition

Van de Ven (1992) defines process as "a sequence of events that describe how things change over time" (p. 169) or "the sequence of incidents, activities, and stages that unfold over the duration of a central subject's [unit's] existence" (p. 170). 


# option 2 Process


# What is Process?

Process is a complex term, and it is helpful to first mention longitudinal designs and then contrast points of emphasis with other, related concepts. 

### Longitudinal

Longitudinal is a term that is used to describe a study's design or data structure. Longitudinal designs collect repeated observations across time, whereas cross-sectional studies do not. Longitudinal data are repeated observations on multiple units, whereas panel data or time series are repeated observations on one unit -- and are the most common data structure in economics. 

### Process

Process refers to the phenomena researchers are exploring when they employ a longitudinal design. Pettigrey (1997) "a sequence of individual and collective events, actions, and activities unfolding over time in context" (p. 2). Van de Ven (1992) "a sequence of events that describes how things change over time" (p. 169). "the sequences of incidents, activities, and stages that unfold over the duration of a central subject's existence" (p. 170). Moreoever, a process may or may not be dynamic and it may or may not contain change.

#### Dynamics

Dynamics refers to a specific branch of mathematics, but the term is used in different ways throughout our literature. It is used informally to mean "change", "fluctuating," "volatile," "longitudinal," or "over time" (among others), whereas formal definitions in our literature are presented within certain contexts. Wang defines a dynamic *model* as a "representation of a system that evolves over time. In particular it describes how the system evolves from a given state at time *t* to another state at time *t + 1* as governed by the transition rules and potential external inputs" (p. 242). Vancouver states that dynamic *variables* "behave as if they have memory; that is, their value at any one time depends somewhat on their previous value" (p. 604). Finally, Monge states that in dynamic *analyses*, "it is essential to know how variables depend upon their own past history" (p. 409). 

The crucial notion to take from dynamics, then, is memory. When the past matters, and future states are constrained by where they were at prior points in time, dynamics are at play. In longitudinal studies that emphasize dynamics, they focus on memory.

#### Change

It is common to emphasis change in longitudinal studies, and some have stated that that is the only point of emphasis. Ployhart and Vandenberg state that "longitudinal research emphasizes the study of change and contains at minimum three repeated observations on at least one of the substantive constructs of interest" (p. 97). Similarly, Baltes and Nesselroade note, "the study of phenomena in their time-related constancy and change is the aim of a longitudinal methodology" (p ?).

But again, process is about sequences of events with or without memory and with or without change. 

# Intro Hype

including emotional labor [@bono2007; @beal2015; @gabriel2015], workplace stress and well-being [@ritter2016; @wang2014], organizatiional performance [@short2006], self-regulation [@hardy2018], newcomer adjustment [@kammeyer-meuller2013], justice and trust [@kaltianinen2017], leadership [@farh2018; @schaubroeck2016; @kalish2016], decision-making [@huang2012], team performance [@humphrey2017], counterproductive work behaviors [@meier2013], work-family conflict [@matthews2014], job satisfaction [@zablah2016], and team emergent states [@kozlowski2012]. In summary, explaining how a process functions appears to be of great interest to current organizational science. 

